{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian_Process():\n",
    "    \n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.gp_ = GaussianProcessRegressor(kernel=kernel,\n",
    "                                            optimizer = None,\n",
    "                                            normalize_y = True)\n",
    "    \n",
    "    def optimise__with_splice_sampling(self, initial_theta, num_iters, sigma, burn_in,  X, y):\n",
    "        self.fit(X, y)\n",
    "        slice_sampler = Slice_sampler(num_iters = num_iters, \n",
    "                                      sigma = sigma, \n",
    "                                      burn_in = burn_in,\n",
    "                                      gp = self)\n",
    "        samples = slice_sampler.sample(init = initial_theta)\n",
    "        \n",
    "        theta_opt = [np.mean(samples_k) for samples_k in samples]\n",
    "        \n",
    "        # kernel__k1__noise_level = noise of the data\n",
    "        # kernel__k2__k1__constant_value = aplitude\n",
    "        # kernel__k2__k2__length_scale = Matern length scales\n",
    "        \n",
    "        self.gp_.set_params(**{\"kernel__k1__noise_level\": np.abs(theta_opt[0]),\n",
    "                              \"kernel__k2__k1__constant_value\": np.abs(theta_opt[1]),\n",
    "                              \"kernel__k2__k2__length_scale\": theta_opt[2:]})\n",
    "        \n",
    "    def log_marginal_likelihood(self,theta):\n",
    "        theta = [theta[0],theta[1],theta[2:]]\n",
    "        return self.gp_.log_marginal_likelihood(theta)\n",
    "    \n",
    "    def log_prior_parameters(self, theta):\n",
    "        return np.sum(np.log([ss.norm(0, 1).pdf(theta_k) for theta_k in theta]))\n",
    "    \n",
    "    def log_joint_unnorm(self, theta):\n",
    "        return self.log_marginal_likelihood(theta) + self.log_prior_parameters(theta)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.gp_.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        mu, sigma = self.gp_.predict(X, return_std=True)\n",
    "        return mu, sigma\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.gp_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Slice_sampler():\n",
    "    \n",
    "    def __init__(self, num_iters, sigma, burn_in, gp):\n",
    "        self.num_iters = num_iters\n",
    "        self.sigma = sigma\n",
    "        self.burn_in = burn_in\n",
    "        self.gp = gp\n",
    "        \n",
    "    def sample(self, init, step_out=True):\n",
    "\n",
    "        D = len(init)\n",
    "        samples = np.zeros((D, self.num_iters))\n",
    "        \n",
    "        xx = init.copy()\n",
    "\n",
    "        for i in xrange(self.num_iters + self.burn_in):\n",
    "            perm = range(D)\n",
    "            np.random.shuffle(perm)\n",
    "            last_llh = self.gp.log_joint_unnorm(xx)\n",
    "\n",
    "            for d in perm:\n",
    "                llh0 = last_llh + np.log(np.random.rand())\n",
    "                rr = np.random.rand(1)\n",
    "                x_l = xx.copy()\n",
    "                x_l[d] = x_l[d] - rr * self.sigma[d]\n",
    "                x_r = xx.copy()\n",
    "                x_r[d] = x_r[d] + (1 - rr) * self.sigma[d]\n",
    "\n",
    "                if step_out:\n",
    "                    llh_l = self.gp.log_joint_unnorm(x_l)\n",
    "                    while llh_l > llh0:\n",
    "                        x_l[d] = x_l[d] - self.sigma[d]\n",
    "                        llh_l = self.gp.log_joint_unnorm(x_l)\n",
    "                    llh_r = self.gp.log_joint_unnorm(x_r)\n",
    "                    while llh_r > llh0:\n",
    "                        x_r[d] = x_r[d] + self.sigma[d]\n",
    "                        llh_r = self.gp.log_joint_unnorm(x_r)\n",
    "\n",
    "                x_cur = xx.copy()\n",
    "                while True:\n",
    "                    xd = np.array(np.random.rand() * (x_r[d] - x_l[d]) + x_l[d])\n",
    "                    x_cur[d] = xd.copy()\n",
    "                    last_llh = self.gp.log_joint_unnorm(x_cur)\n",
    "                    if last_llh > llh0:\n",
    "                        xx[d] = xd.copy()\n",
    "                        break\n",
    "                    elif xd > xx[d]:\n",
    "                        x_r[d] = xd\n",
    "                    elif xd < xx[d]:\n",
    "                        x_l[d] = xd\n",
    "                    else:\n",
    "                        raise RuntimeError('Slice sampler shrank too far.')\n",
    "                \n",
    "            if i == 0:\n",
    "                print \"burn-in\"\n",
    "            elif i > self.burn_in and i % 100 == 0: \n",
    "                print 'iteration', i - self.burn_in\n",
    "            \n",
    "            if i > self.burn_in:   \n",
    "                samples[:, i-self.burn_in] = xx.copy().ravel()\n",
    "        \n",
    "        #plt.hist(samples[0])\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_features = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.linspace(-5,5,50)\n",
    "# y_train = np.sin(X_train) + np.random.normal(loc = 0, scale = 0.5,size=50)\n",
    "# plt.plot(X_train, y_train, \"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = kernels.Sum(kernels.WhiteKernel(),kernels.Product(kernels.ConstantKernel(),kernels.Matern(nu=5./2.)))\n",
    "gp = Gaussian_Process(kernel = kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcodelvecchio/miniconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burn-in\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n"
     ]
    }
   ],
   "source": [
    "gp.optimise__with_splice_sampling(initial_theta = np.array([1.,1.,1.,1.,1.]),\n",
    "                                  num_iters = 1000,\n",
    "                                  sigma = np.array([1.,1.,1.,1.,1.]),\n",
    "                                  burn_in = 200,\n",
    "                                  X = X_train,\n",
    "                                  y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-10,\n",
       " 'copy_X_train': True,\n",
       " 'kernel': WhiteKernel(noise_level=15.1) + 4.02**2 * Matern(length_scale=[6.32, 6.16, 6.59], nu=2.5),\n",
       " 'kernel__k1': WhiteKernel(noise_level=15.1),\n",
       " 'kernel__k1__noise_level': 15.069799000172413,\n",
       " 'kernel__k1__noise_level_bounds': (1e-05, 100000.0),\n",
       " 'kernel__k2': 4.02**2 * Matern(length_scale=[6.32, 6.16, 6.59], nu=2.5),\n",
       " 'kernel__k2__k1': 4.02**2,\n",
       " 'kernel__k2__k1__constant_value': 16.171890684789869,\n",
       " 'kernel__k2__k1__constant_value_bounds': (1e-05, 100000.0),\n",
       " 'kernel__k2__k2': Matern(length_scale=[6.32, 6.16, 6.59], nu=2.5),\n",
       " 'kernel__k2__k2__length_scale': [6.3215492419657435,\n",
       "  6.1580030812523994,\n",
       "  6.5910779650128442],\n",
       " 'kernel__k2__k2__length_scale_bounds': (1e-05, 100000.0),\n",
       " 'kernel__k2__k2__nu': 2.5,\n",
       " 'n_restarts_optimizer': 0,\n",
       " 'normalize_y': True,\n",
       " 'optimizer': None,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = gp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88719673303209945"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true = y_test, y_pred = mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(X_test, mu)\n",
    "#plt.plot(X_test, mu+sigma)\n",
    "#plt.plot(X_test, mu-sigma)\n",
    "#plt.plot(X_train, y_train, \"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
