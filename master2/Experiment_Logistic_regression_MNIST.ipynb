{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from gp_master import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mean_std(y_list):\n",
    "    mean_list = list()\n",
    "    std_list = list()\n",
    "    for one_list in y_list:\n",
    "        mean_list.append(np.mean(one_list))\n",
    "        std_list.append(np.std(one_list))\n",
    "    return mean_list, std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logisitc_regression_(mnist):\n",
    "    def train_logisitc_regression_(params):\n",
    "        learning_rate_log = params[0]\n",
    "        training_epochs = int(params[1])\n",
    "        batch_size = int(params[2])\n",
    "        beta = params[3]\n",
    "        learning_rate = np.exp(learning_rate_log)\n",
    "        print \"\\tLearning rate: \" + str(learning_rate) + \", training epochs: \" + str(training_epochs) + \", batch size: \"+ str(batch_size) + \", beta \" + str(beta)\n",
    "        display_step = 10\n",
    "        # tf Graph Input\n",
    "        x = tf.placeholder(tf.float32, [None, 784]) \n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        # Set model weights\n",
    "        W = tf.Variable(tf.zeros([784, 10]))\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "        # Construct model\n",
    "        pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "        # Create regulariser\n",
    "        regularizer = tf.nn.l2_loss(W)\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1) + beta * regularizer)\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        # Initialize the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                                  y: batch_ys})\n",
    "                    # Compute average loss\n",
    "                    avg_cost += c / total_batch\n",
    "                # Display logs per epoch step\n",
    "                #if (epoch+1) % display_step == 0:\n",
    "                #    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            # Test model\n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            accuracy_result = accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n",
    "            print \"\\t\\t Accuracy: \" + str(accuracy_result)\n",
    "        return 1 - accuracy_result\n",
    "    return train_logisitc_regression_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_logisitc_regression_(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "bounds = np.array([[0, 1], [5, 2000], [20, 2000], [0, 1]])\n",
    "# maxeps specifies how many BOs we want to run\n",
    "maxeps = 5\n",
    "# n_iters is number of iterations we want for each run of BO\n",
    "n_iters = 100\n",
    "acqui_eva_num = 3\n",
    "n_pre_samples = 3\n",
    "y_list = list()\n",
    "slice_sample_num = 1\n",
    "coor_sigma = 5 * np.array([0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "burn_in = 3\n",
    "input_dimension = 4\n",
    "mode = 'MAP'\n",
    "acqui_mode = 'MCMC'\n",
    "acqui_sample_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 episode\n",
      "\tLearning rate: 2.4868401009249474, training epochs: 731, batch size: 287, beta 0.11578367394696043\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.3824624163877581, training epochs: 921, batch size: 886, beta 0.4840663403794334\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.7911901370212413, training epochs: 1704, batch size: 377, beta 0.4622985386641666\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.127037331230966, training epochs: 1289, batch size: 34, beta 0.8316573754448001\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.1291327183914777, training epochs: 212, batch size: 1549, beta 0.2034971107591509\n",
      "\t\t Accuracy: 0.6213\n",
      "\tLearning rate: 2.5513577194968833, training epochs: 871, batch size: 1729, beta 0.19176367076569556\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.932241615665274, training epochs: 1134, batch size: 1618, beta 0.2973674312356921\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0256952117288383, training epochs: 824, batch size: 516, beta 0.8151859793519179\n",
      "\t\t Accuracy: 0.2165\n",
      "\tLearning rate: 1.015924402534768, training epochs: 1801, batch size: 372, beta 0.8944116827456177\n",
      "\t\t Accuracy: 0.3487\n",
      "\tLearning rate: 2.5589406697019017, training epochs: 546, batch size: 989, beta 0.08518796218255764\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.7431343176869298, training epochs: 1066, batch size: 809, beta 0.9165351069504617\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.4637618330333764, training epochs: 194, batch size: 389, beta 0.40702721621249305\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.3241518634919054, training epochs: 1311, batch size: 1331, beta 0.16289702508636428\n",
      "\t\t Accuracy: 0.4068\n",
      "\tLearning rate: 1.7434644968870303, training epochs: 1675, batch size: 1935, beta 0.8930324562845412\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.6013959362254897, training epochs: 1592, batch size: 1427, beta 0.23234726734887612\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.9758914668795367, training epochs: 726, batch size: 1011, beta 0.33831245086654316\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.4588767817851587, training epochs: 142, batch size: 158, beta 0.8308353379490532\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.2613773726390407, training epochs: 1892, batch size: 149, beta 0.6576886093927469\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.4623135555218023, training epochs: 1175, batch size: 1028, beta 0.793146369990818\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.4542825520874536, training epochs: 1414, batch size: 1653, beta 0.624493613655718\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.588394764463947, training epochs: 1742, batch size: 320, beta 0.5460460904283205\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.5387400302457306, training epochs: 1496, batch size: 498, beta 0.12173286789093152\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.4790576298047609, training epochs: 24, batch size: 1106, beta 0.3231115994268545\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.6711917520634272, training epochs: 567, batch size: 1086, beta 0.1757907069017588\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.489651065866544, training epochs: 1244, batch size: 1313, beta 0.6839337443300841\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0546211525215765, training epochs: 1821, batch size: 1660, beta 0.035338671362117324\n",
      "\t\t Accuracy: 0.72\n",
      "\tLearning rate: 1.6906781439755232, training epochs: 79, batch size: 1361, beta 0.5465136838448932\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.0104018521107543, training epochs: 57, batch size: 1312, beta 0.9102002600448844\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.1745484385167317, training epochs: 1148, batch size: 1085, beta 0.9481687609250271\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0693432524171254, training epochs: 310, batch size: 482, beta 0.5929099083738707\n",
      "\t\t Accuracy: 0.3054\n",
      "\tLearning rate: 1.8753498255480612, training epochs: 649, batch size: 830, beta 0.3579975144995887\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.461032405761189, training epochs: 452, batch size: 351, beta 0.2835425501051059\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.2491762106051245, training epochs: 1423, batch size: 1078, beta 0.7389570289274919\n",
      "\t\t Accuracy: 0.3673\n",
      "\tLearning rate: 1.8600639528312433, training epochs: 1027, batch size: 1030, beta 0.0997061556004557\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.3167838095952358, training epochs: 32, batch size: 192, beta 0.8017046833555231\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.396808496797717, training epochs: 899, batch size: 224, beta 0.060030491159083876\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.4703238762675768, training epochs: 702, batch size: 247, beta 0.8733425540235668\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.1629512609831525, training epochs: 1444, batch size: 1484, beta 0.9657441347822563\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.154045896473614, training epochs: 425, batch size: 466, beta 0.3618455071896739\n",
      "\t\t Accuracy: 0.463\n",
      "\tLearning rate: 1.7475189636947575, training epochs: 1981, batch size: 224, beta 0.8219735057942963\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.1095376248007434, training epochs: 1320, batch size: 1126, beta 0.3065192091715362\n",
      "\t\t Accuracy: 0.3241\n",
      "\tLearning rate: 2.4882867113773557, training epochs: 1329, batch size: 1636, beta 0.38680258361519615\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.442932779354408, training epochs: 771, batch size: 769, beta 0.7293269704069318\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.2908926763068438, training epochs: 593, batch size: 58, beta 0.49273005524500435\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.6960808607401916, training epochs: 1628, batch size: 730, beta 0.9660054644940033\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0121028891881014, training epochs: 1524, batch size: 292, beta 0.8835702266617925\n",
      "\t\t Accuracy: 0.3158\n",
      "\tLearning rate: 1.094353654826195, training epochs: 546, batch size: 1083, beta 0.3897090993400286\n",
      "\t\t Accuracy: 0.3174\n",
      "\tLearning rate: 1.5208871413193927, training epochs: 462, batch size: 1957, beta 0.6556739170733015\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.3981579444499115, training epochs: 464, batch size: 1548, beta 0.4568191534452706\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.1329600540982283, training epochs: 851, batch size: 353, beta 0.385331229656519\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.3283135685815175, training epochs: 669, batch size: 507, beta 0.6879958736915686\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.503150467725364, training epochs: 1877, batch size: 1768, beta 0.7967282620518603\n",
      "\t\t Accuracy: 0.098\n",
      "50 iterations have been run\n",
      "\tLearning rate: 1.447572733075959, training epochs: 145, batch size: 1982, beta 0.4506934684870787\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.9085880924619654, training epochs: 1521, batch size: 1163, beta 0.3366192016140249\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0962039233794507, training epochs: 1702, batch size: 1950, beta 0.5099145316133391\n",
      "\t\t Accuracy: 0.2808\n",
      "\tLearning rate: 1.5845620925228683, training epochs: 1223, batch size: 1011, beta 0.23715151780581467\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.031292202060044, training epochs: 517, batch size: 288, beta 0.9498157044932015\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.6547452246304104, training epochs: 1869, batch size: 1610, beta 0.4170197689126425\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.903424686097329, training epochs: 598, batch size: 1456, beta 0.4329602577613926\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.082242762071775, training epochs: 1716, batch size: 1255, beta 0.9457574034162041\n",
      "\t\t Accuracy: 0.3566\n",
      "\tLearning rate: 1.1767621566242001, training epochs: 838, batch size: 598, beta 0.8105449757855824\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.219783978485482, training epochs: 866, batch size: 1153, beta 0.6085799938949811\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.9557754974980532, training epochs: 86, batch size: 1856, beta 0.8637429184348215\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0509639322570494, training epochs: 1435, batch size: 36, beta 0.8197194951247219\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.1288800716149754, training epochs: 107, batch size: 251, beta 0.8662450410722856\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.371651486380706, training epochs: 1433, batch size: 1982, beta 0.00944334189823648\n",
      "\t\t Accuracy: 0.8338\n",
      "\tLearning rate: 1.1613616607352304, training epochs: 1765, batch size: 456, beta 0.44201828950608146\n",
      "\t\t Accuracy: 0.4081\n",
      "\tLearning rate: 2.2321780060003054, training epochs: 550, batch size: 1106, beta 0.7055103945542183\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0256102510151741, training epochs: 913, batch size: 761, beta 0.8329278707926615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Accuracy: 0.4232\n",
      "\tLearning rate: 2.2885412159232454, training epochs: 1915, batch size: 1156, beta 0.5054510813291629\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0646850216215085, training epochs: 637, batch size: 947, beta 0.6647270362943869\n",
      "\t\t Accuracy: 0.4498\n",
      "\tLearning rate: 2.351911575770049, training epochs: 657, batch size: 1973, beta 0.0430926451570659\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.7733921650899138, training epochs: 922, batch size: 1420, beta 0.6120100464812771\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.321764722115425, training epochs: 1367, batch size: 304, beta 0.5777148282190823\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.41326880454705, training epochs: 1168, batch size: 1283, beta 0.2742678000097314\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.9351436254625964, training epochs: 668, batch size: 1518, beta 0.9204159439361123\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.5908166186071204, training epochs: 1235, batch size: 446, beta 0.5099194851173643\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.8817370971448182, training epochs: 82, batch size: 1960, beta 0.8948066969466367\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.5824259559072538, training epochs: 1394, batch size: 426, beta 0.8067499171888979\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.0918361951544577, training epochs: 715, batch size: 1956, beta 0.25714157213746913\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.1092180870262498, training epochs: 863, batch size: 699, beta 0.30280905581943285\n",
      "\t\t Accuracy: 0.4415\n",
      "\tLearning rate: 1.3723490864940224, training epochs: 1217, batch size: 1367, beta 0.1798566052841295\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0434286700239301, training epochs: 249, batch size: 1819, beta 0.4360980222970443\n",
      "\t\t Accuracy: 0.4071\n",
      "\tLearning rate: 1.2312696706669424, training epochs: 1998, batch size: 732, beta 0.09518547783470621\n",
      "\t\t Accuracy: 0.6091\n",
      "\tLearning rate: 1.9021219646648273, training epochs: 1262, batch size: 1948, beta 0.4369945448479766\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.3939095505936117, training epochs: 1320, batch size: 1320, beta 0.9394907881444795\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.3969738333528856, training epochs: 555, batch size: 76, beta 0.8135112979395003\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 2.4177556282938566, training epochs: 17, batch size: 1925, beta 0.5731134653013031\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.0926402342655628, training epochs: 210, batch size: 1304, beta 0.4048901352414237\n",
      "\t\t Accuracy: 0.2505\n",
      "\tLearning rate: 2.3893619012167826, training epochs: 1470, batch size: 1441, beta 0.5300075259413384\n",
      "\t\t Accuracy: 0.098\n",
      "\tLearning rate: 1.6909495839174853, training epochs: 1061, batch size: 670, beta 0.015281874538305651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9965430f4e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                     \u001b[0macqui_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macqui_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                     acqui_sample_num = acqui_sample_num)\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_pre_samples\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/remote/mlsalt-2017/yz551/MLSALT4/Bayesian-opt/master/gp_master.py\u001b[0m in \u001b[0;36mbayesian_optimisation\u001b[0;34m(slice_sample_num, coor_sigma, burn_in, input_dimension, n_iters, sample_loss, bounds, x0, n_pre_samples, acqui_eva_num, random_search, epsilon, greater_is_better, mode, acqui_mode, acqui_sample_num)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mfunc_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;31m# Update lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b6063941bb1a>\u001b[0m in \u001b[0;36mtrain_logisitc_regression_\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n\u001b[0;32m---> 39\u001b[0;31m                                                                   y: batch_ys})\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_iters):\n",
    "    y_list.append(list())\n",
    "\n",
    "\n",
    "for j in range(maxeps):\n",
    "    print ('Running %d episode' % (j + 1))\n",
    "    xp, yp, timep  = bayesian_optimisation(slice_sample_num, \n",
    "                                    coor_sigma, \n",
    "                                    burn_in, \n",
    "                                    input_dimension,\n",
    "                                    n_iters=n_iters, \n",
    "                                    sample_loss=loss, \n",
    "                                    bounds=bounds,\n",
    "                                    n_pre_samples=n_pre_samples,\n",
    "                                    acqui_eva_num = acqui_eva_num,\n",
    "                                    random_search=False,\n",
    "                                    greater_is_better = False,\n",
    "                                    mode = mode,\n",
    "                                    acqui_mode = acqui_mode,\n",
    "                                    acqui_sample_num = acqui_sample_num)\n",
    "    for idx in range(n_iters):\n",
    "        y_list[idx].append(np.min(yp[:idx+n_pre_samples+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list, std_list = return_mean_std(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(len(y_list))]\n",
    "plt.errorbar(x, mean_list, yerr = std_list, fmt = '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "bounds = np.array([[0, 1], [5, 2000], [20, 2000], [0, 1]])\n",
    "# maxeps specifies how many BOs we want to run\n",
    "maxeps = 10\n",
    "# n_iters is number of iterations we want for each run of BO\n",
    "n_iters = 100\n",
    "acqui_eva_num = 3\n",
    "n_pre_samples = 3\n",
    "y_list = list()\n",
    "slice_sample_num = 1\n",
    "coor_sigma = 5 * np.array([0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "burn_in = 3\n",
    "input_dimension = 4\n",
    "mode = 'OPT'\n",
    "acqui_mode = 'OPT'\n",
    "acqui_sample_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iters):\n",
    "    y_list.append(list())\n",
    "\n",
    "for j in range(maxeps):\n",
    "    print ('Running %d episode' % (j + 1))\n",
    "    xp, yp = bayesian_optimisation(slice_sample_num, \n",
    "                                   coor_sigma, \n",
    "                                   burn_in, \n",
    "                                   input_dimension,\n",
    "                                   n_iters=n_iters, \n",
    "                                   sample_loss=loss, \n",
    "                                   bounds=bounds,\n",
    "                                   n_pre_samples=n_pre_samples,\n",
    "                                   acqui_eva_num = acqui_eva_num,\n",
    "                                   random_search=False,\n",
    "                                   greater_is_better = False,\n",
    "                                   mode = mode,\n",
    "                                   acqui_mode = acqui_mode,\n",
    "                                   acqui_sample_num = acqui_sample_num)\n",
    "    for idx in range(n_iters):\n",
    "        y_list[idx].append(np.min(yp[:idx+n_pre_samples+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list, std_list = return_mean_std(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(len(y_list))]\n",
    "plt.errorbar(x, mean_list, yerr = std_list, fmt = '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
