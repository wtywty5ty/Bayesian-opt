{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from gp_master import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mean_std(y_list):\n",
    "    mean_list = list()\n",
    "    std_list = list()\n",
    "    for one_list in y_list:\n",
    "        mean_list.append(np.mean(one_list))\n",
    "        std_list.append(np.std(one_list))\n",
    "    return mean_list, std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logisitc_regression_(mnist):\n",
    "    def train_logisitc_regression_(params):\n",
    "        learning_rate = params[0]\n",
    "        training_epochs = int(params[1])\n",
    "        batch_size = int(params[2])\n",
    "        beta = params[3]\n",
    "        print \"\\tLearning rate: \" + str(learning_rate) + \", training epochs: \" + str(training_epochs) + \", batch size: \"+ str(batch_size) + \", beta \" + str(beta)\n",
    "        display_step = 10\n",
    "        # tf Graph Input\n",
    "        x = tf.placeholder(tf.float32, [None, 784]) \n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        # Set model weights\n",
    "        W = tf.Variable(tf.zeros([784, 10]))\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "        # Construct model\n",
    "        pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "        # Create regulariser\n",
    "        regularizer = tf.nn.l2_loss(W)\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1) + beta * regularizer)\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        # Initialize the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                                  y: batch_ys})\n",
    "                    # Compute average loss\n",
    "                    avg_cost += c / total_batch\n",
    "                # Display logs per epoch step\n",
    "                #if (epoch+1) % display_step == 0:\n",
    "                #    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            # Test model\n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            accuracy_result = accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n",
    "            print \"\\t\\t Accuracy: \" + str(accuracy_result)\n",
    "        return 1 - accuracy_result\n",
    "    return train_logisitc_regression_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_logisitc_regression_(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "bounds = np.array([[0, 1], [5, 2000], [20, 2000], [0, 1]])\n",
    "# maxeps specifies how many BOs we want to run\n",
    "maxeps = 5\n",
    "# n_iters is number of iterations we want for each run of BO\n",
    "n_iters = 100\n",
    "acqui_eva_num = 3\n",
    "n_pre_samples = 3\n",
    "y_list = list()\n",
    "slice_sample_num = 1\n",
    "coor_sigma = 5 * np.array([0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "burn_in = 3\n",
    "input_dimension = 4\n",
    "mode = 'MAP'\n",
    "acqui_mode = 'MCMC'\n",
    "acqui_sample_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iters):\n",
    "    y_list.append(list())\n",
    "\n",
    "\n",
    "for j in range(maxeps):\n",
    "    print ('Running %d episode' % (j + 1))\n",
    "    xp, yp  = bayesian_optimisation(slice_sample_num, \n",
    "                                    coor_sigma, \n",
    "                                    burn_in, \n",
    "                                    input_dimension,\n",
    "                                    n_iters=n_iters, \n",
    "                                    sample_loss=loss, \n",
    "                                    bounds=bounds,\n",
    "                                    n_pre_samples=n_pre_samples,\n",
    "                                    acqui_eva_num = acqui_eva_num,\n",
    "                                    random_search=False,\n",
    "                                    greater_is_better = False,\n",
    "                                    mode = mode,\n",
    "                                    acqui_mode = acqui_mode,\n",
    "                                    acqui_sample_num = acqui_sample_num)\n",
    "    for idx in range(n_iters):\n",
    "        y_list[idx].append(np.min(yp[:idx+n_pre_samples+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list, std_list = return_mean_std(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(len(y_list))]\n",
    "plt.errorbar(x, mean_list, yerr = std_list, fmt = '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "bounds = np.array([[0, 1], [5, 2000], [20, 2000], [0, 1]])\n",
    "# maxeps specifies how many BOs we want to run\n",
    "maxeps = 10\n",
    "# n_iters is number of iterations we want for each run of BO\n",
    "n_iters = 100\n",
    "acqui_eva_num = 3\n",
    "n_pre_samples = 3\n",
    "y_list = list()\n",
    "slice_sample_num = 1\n",
    "coor_sigma = 5 * np.array([0.2,0.2,0.2,0.2,0.2,0.2])\n",
    "burn_in = 3\n",
    "input_dimension = 4\n",
    "mode = 'OPT'\n",
    "acqui_mode = 'OPT'\n",
    "acqui_sample_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_iters):\n",
    "    y_list.append(list())\n",
    "\n",
    "for j in range(maxeps):\n",
    "    print ('Running %d episode' % (j + 1))\n",
    "    xp, yp = bayesian_optimisation(slice_sample_num, \n",
    "                                   coor_sigma, \n",
    "                                   burn_in, \n",
    "                                   input_dimension,\n",
    "                                   n_iters=n_iters, \n",
    "                                   sample_loss=loss, \n",
    "                                   bounds=bounds,\n",
    "                                   n_pre_samples=n_pre_samples,\n",
    "                                   acqui_eva_num = acqui_eva_num,\n",
    "                                   random_search=False,\n",
    "                                   greater_is_better = False,\n",
    "                                   mode = mode,\n",
    "                                   acqui_mode = acqui_mode,\n",
    "                                   acqui_sample_num = acqui_sample_num)\n",
    "    for idx in range(n_iters):\n",
    "        y_list[idx].append(np.min(yp[:idx+n_pre_samples+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list, std_list = return_mean_std(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(len(y_list))]\n",
    "plt.errorbar(x, mean_list, yerr = std_list, fmt = '-o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
